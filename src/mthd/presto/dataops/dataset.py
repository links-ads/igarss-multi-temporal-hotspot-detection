import logging
import os
import random
import shutil
from collections import defaultdict
from pathlib import Path
from sys import platform
from typing import Any, Dict, Iterable, List, Optional, Tuple

import ee
import geopandas as gpd
import numpy as np
import pandas as pd
import webdataset as wds
import yaml
from google.cloud import storage
from hurry.filesize import size
from openmapflow.ee_boundingbox import EEBoundingBox
from shapely import geometry
from tqdm import tqdm

from .. import utils
from .masking import MaskedExample, MaskParams
from .pipelines.dynamicworld import DynamicWorldMonthly2020_2021, pad_array
from .pipelines.ee_pipeline import EE_BUCKET, NPY_BUCKET, EEPipeline
from .pipelines.s1_s2_era5_srtm import NUM_TIMESTEPS, S1_S2_ERA5_SRTM_2020_2021
from .pipelines.worldcover2020 import WorldCover2020

METRES_PER_PATCH = 50000  # Ensures EarthEngine exports don't time out
TAR_BUCKET = "lem-assets2"
N_RECORDS_IN_SHARD = 144

logger = logging.getLogger("__main__")


class Dataset:
    """
    The Dataset class is used to access and generate all training data.
    1) Data is generated by using data pipelines (EEPipeline)
    2) Once generated, data is packaged into webdataset tar file and stored on GCS
    3) Data is accessed by using the as_webdataset() method which returns an Iterable
    """

    tar_command_dict = {"linux": "tar", "linux2": "tar", "darwin": "gtar"}

    def __init__(self, pipelines: Tuple[EEPipeline, ...]):
        self.pipelines = pipelines
        self.name = "_".join([p.name for p in self.pipelines])
        self._create_webdataset_tar_prereqs_checked = False
        self.existing_tars: Optional[List[str]] = None

    def _check_prereqs(self):
        if not self._create_webdataset_tar_prereqs_checked:
            tar_command = self.tar_command_dict[platform]
            if platform == "win32":
                raise EnvironmentError("Windows is not supported")
            if shutil.which("gcloud") is None:
                raise EnvironmentError("gcloud is not installed")
            if shutil.which(tar_command) is None:
                raise EnvironmentError(f"{tar_command} is not installed")
            self._create_webdataset_tar_prereqs_checked = True

    def _does_tar_already_exist(self, prefix: str, force: bool = False):
        if self.existing_tars is None or force:
            self.existing_tars = [blob.name for blob in storage.Client().list_blobs(TAR_BUCKET)]
        if not isinstance(self.existing_tars, list):
            raise TypeError(f"Expected list but got {type(self.existing_tars)}")
        return f"{self.name}_tars/{prefix}.tar" in self.existing_tars

    def _create_tar(
        self,
        prefix: str,
        polygon_ids: List[str],
    ):
        """Creates tar from npy files from Google Cloud Storage"""
        tar_dir = Path(prefix)
        tar_dir.mkdir(exist_ok=True)

        logger.info("Downloading npy files")
        # Download numpy files (faster than using Python API)
        os.system(f"gcloud storage cp -n -r gs://{NPY_BUCKET}/{prefix} .")

        # Download numpy files
        npy_files = list(tar_dir.glob("**/*.npy"))
        if len(npy_files) == 0:
            raise FileNotFoundError(f"No npy files found in {tar_dir}")
        for p in tqdm(npy_files, desc=f"{prefix} renaming npy files", leave=True):
            clean_name = str(p).replace("-retry", "").replace(".npy", "")
            if len(clean_name.split("/")) != 4:
                raise ValueError(
                    f"Expected 3 '/' characters in: {clean_name}. "
                    + "This is likely caused because previous file processing was interrupted. "
                    + "Delete local copies and rerun the script."
                )

            actual_prefix, actual_id, pipeline_name, stem = clean_name.split("/")
            if actual_prefix != prefix:
                raise ValueError(f"Expected {prefix} but got {actual_prefix}")
            if actual_id not in polygon_ids:
                raise ValueError(f"{actual_id} not found in {polygon_ids}")
            flattened_file = tar_dir / f"{actual_id.replace('.', '_')}_{stem}.{pipeline_name}.npy"
            p.rename(flattened_file)

        # Create tar file
        tar_command = self.tar_command_dict[platform]
        os.system(f"{tar_command} --sort=name -cf {prefix}.tar {prefix}/")
        shutil.rmtree(tar_dir)

        # Upload to Google Cloud Storage
        os.system(
            f"gcloud storage cp {prefix}.tar gs://{TAR_BUCKET}/{self.name}_tars/{prefix}.tar"
        )

        if self._does_tar_already_exist(prefix, force=True):
            Path(f"{prefix}.tar").unlink()
        else:
            raise Exception(f"Tar file {prefix}.tar was not uploaded to GCS.")

    def _create_webdataset_tar_from_small_polygons(
        self, polygons: List[geometry.Polygon], polygon_ids: List[str], prefix: str
    ):
        """
        Idempotent function for creating a webdataset tar file
        from the given large polygon and polygon prefix.

        Recommended polygon sizes: < (5k x 5k pixels) 50x50 km at 10 m resolution
        """

        self._check_prereqs()

        client = storage.Client()
        if not ee.data._credentials:
            ee.Initialize()

        n_polygon_ids = len(set(polygon_ids))

        pipeline_names = [p.name for p in self.pipelines]

        def status(ee_polygon_ids, npy_polygon_ids, n_tars: int):
            text = ("-" * 60) + f"\n{prefix} ({self.name})\n" + ("-" * 60) + "\n"
            text += " Earth Engine files:\n"
            for p in pipeline_names:
                text += f" \t{p}: {len(ee_polygon_ids[p])}/{n_polygon_ids}\n"
            text += " Numpy processing:\n"
            for p in pipeline_names:
                text += f" \t{p}: {len(npy_polygon_ids[p])}/{n_polygon_ids}\n"
            text += f" Complete tar: {n_tars}/1\n"
            logger.info(text)
            return text

        if self._does_tar_already_exist(prefix):
            mock_polygon_ids = {p: set(polygon_ids) for p in pipeline_names}
            return status(mock_polygon_ids, mock_polygon_ids, 1)

        def get_pipeline_polygon_ids_dict(bucket: str):
            polygon_ids = defaultdict(lambda: set())
            for b in client.list_blobs(bucket, prefix=prefix + "/"):
                pipeline_name = b.name.split("/")[2]
                if pipeline_name in pipeline_names:
                    polygon_ids[pipeline_name].add(b.name.split("/")[1])
            return polygon_ids

        ee_polygon_ids = get_pipeline_polygon_ids_dict(EE_BUCKET)
        npy_polygon_ids = get_pipeline_polygon_ids_dict(NPY_BUCKET)

        missing_polygons = []

        # S1_S2_ERA5_SRTM_2020_2021 will not always be exported (S2 coverage),
        # so it can't be used to check for missing polygons
        pipeline_names_to_check = [p for p in pipeline_names if p != "S1_S2_ERA5_SRTM_2020_2021"]
        for p_id in polygon_ids:
            if any([p_id not in npy_polygon_ids[p] for p in pipeline_names_to_check]):
                missing_polygons.append(p_id)

        if len(missing_polygons) == 0:
            status(ee_polygon_ids, npy_polygon_ids, 0)
            self._create_tar(prefix, polygon_ids)
            return status(ee_polygon_ids, npy_polygon_ids, 1)

        for polygon, polygon_id in tqdm(list(zip(polygons, polygon_ids))):
            for p in self.pipelines:
                if polygon_id not in ee_polygon_ids[p.name]:
                    min_lon, min_lat, max_lon, max_lat = polygon.exterior.bounds
                    ee_polygon = EEBoundingBox(min_lat, max_lat, min_lon, max_lon).to_ee_polygon()
                    p.run(ee_polygon, prefix=f"{prefix}/{polygon_id}")

        return status(ee_polygon_ids, npy_polygon_ids, 0)

    def _create_webdataset_tar_from_big_polygon(self, polygon: geometry.Polygon, prefix: str):
        """
        Idempotent function for creating a webdataset tar file
        from the given large polygon and polygon prefix.

        Recommended polygon size: > (5k x 5k pixels) 50x50 km at 10 m resolution
        """
        if type(polygon) == geometry.MultiPolygon:
            raise ValueError("MultiPolygon not supported")
        self._check_prereqs()

        min_lon, min_lat, max_lon, max_lat = polygon.exterior.bounds
        if not ee.data._credentials:
            ee.Initialize()
        ee_bbox = EEBoundingBox(min_lat, max_lat, min_lon, max_lon)
        ee_polygons = ee_bbox.to_polygons(metres_per_patch=METRES_PER_PATCH)

        client = storage.Client()

        status_exporting = 0
        status_npy_processing = 0
        status_tar_complete = 0
        for i, ee_polygon in enumerate(tqdm(ee_polygons, desc=prefix)):
            tar_name = f"{prefix}_{i}"
            if self._does_tar_already_exist(tar_name):
                status_tar_complete += 1
                continue

            npy_blobs_from_pipelines = []
            for p in self.pipelines:
                npy_blobs = p.run(ee_polygon, prefix=f"{prefix}/{i}")
                npy_blobs_from_pipelines.append(npy_blobs)

            lat_lon_prefix = f"{prefix}/{i}/LatLon/"
            lat_lon_npy_blobs = list(client.list_blobs(NPY_BUCKET, prefix=lat_lon_prefix))
            npy_blobs_from_pipelines.append(lat_lon_npy_blobs)

            npy_blob_amounts = [len(blobs) for blobs in npy_blobs_from_pipelines]
            if npy_blob_amounts[0] == 0:
                status_exporting += 1
                continue

            if not all(npy_blob_amounts[0] == amt for amt in npy_blob_amounts):
                status_npy_processing += 1
                continue

            npy_blobs = [blob for blobs in npy_blobs_from_pipelines for blob in blobs]
            self._create_tar(tar_name, [tar_name])
            status_tar_complete += 1

        status_text = ("-" * 60) + f"\n{prefix} ({self.name})\n" + ("-" * 60) + "\n"
        if status_exporting > 0:
            status_checks = "\u2715", "\u2715", "\u2715"
        elif status_npy_processing > 0:
            status_checks = "\u2714", "\u2715", "\u2715"
        else:
            status_checks = "\u2714", "\u2714", "\u2714"
        status_text += f"{status_checks[0]} Earth Engine export: {status_exporting}\n"
        status_text += f"{status_checks[1]} Numpy processing: {status_npy_processing}\n"
        status_text += f"{status_checks[2]} Complete tars: {status_tar_complete}\n"
        logger.info(status_text)
        return status_text

    def _decode_file_from_webdataset_tar(self):
        return wds.decode()

    def _tuples_from_decoded_files(self, iter: Iterable[Dict[str, Any]]):
        raise NotImplementedError

    def as_webdataset(self, url: str, shuffle=True, seed: int = 42):
        rng = random.Random()
        rng.seed(seed)

        steps = [
            wds.SimpleShardList(
                f"pipe:gcloud storage cat {url}" if url.startswith("gs://") else url
            ),
            wds.cached_tarfile_to_samples(cache_dir=f"{utils.data_dir}/tars/{Path(url).stem}"),
        ]

        if shuffle:
            steps.append(wds.shuffle(1000, rng=rng))  # Shuffles samples inside tarfile

        steps.append(self._decode_file_from_webdataset_tar())
        steps.append(self._tuples_from_decoded_files)

        if shuffle:
            steps.append(wds.shuffle(1000, rng=rng))  # Shuffles pixels from one sample

        return wds.DataPipeline(steps)

    @staticmethod
    def clip(x: np.ndarray, start_month: int):
        if x.shape[1] > start_month:
            return x[:, start_month : start_month + NUM_TIMESTEPS]
        else:
            # sometimes, we don't have enough dynamic world
            # timesteps. This should be fixed with the new exporter.
            return x[:, :NUM_TIMESTEPS]


class S1_S2_ERA5_SRTM_DynamicWorld_WorldCover_2020_2021(Dataset):
    def __init__(
        self,
        mask_params: Optional[MaskParams] = None,
    ):
        self.eo_pipeline = S1_S2_ERA5_SRTM_2020_2021()
        self.dynamicworld_pipeline = DynamicWorldMonthly2020_2021()
        self.worldcover_pipeline = WorldCover2020()
        self.mask_params = mask_params
        super().__init__(
            pipelines=(self.eo_pipeline, self.dynamicworld_pipeline, self.worldcover_pipeline)
        )

    def _generate_stats(self):
        """
        Generates data/tile_stats.yaml containing metadata (class sizes, file amounts)
        for each available tile.
        """
        tar_folders = [f"{self.name}_tars/"]

        class_df = pd.read_csv(f"https://storage.googleapis.com/{TAR_BUCKET}/esa_grid.csv")
        class_df = class_df.drop(["geometry"], axis=1)
        esa_grid = gpd.read_file(utils.data_dir / "esa_worldcover_2020_grid.geojson")
        df = esa_grid.merge(class_df, on="ll_tile", how="left")

        tar_stats = {}

        for tar_folder in tar_folders:
            tile_stats = defaultdict(lambda: {"num_files": 0, "size_bytes": 0, "classes": {}})

            for blob in storage.Client().list_blobs(TAR_BUCKET, prefix=tar_folder):
                tile_name, _ = Path(blob.name).stem.split("_")
                tile_stats[tile_name]["num_files"] += 1
                tile_stats[tile_name]["size_bytes"] += blob.size

            for tile in list(tile_stats.keys()):
                # Readable tile size
                tile_stats[tile]["size"] = size(tile_stats[tile]["size_bytes"])

                row = df[df["ll_tile"] == tile].iloc[0].to_dict()

                lon, lat = row["geometry"].centroid.coords[0]
                gmap_url = f"http://maps.google.com/maps?z=6&t=k&q=loc:{lat}+{lon}"
                tile_stats[tile]["gmap_url"] = gmap_url

                for k, v in row.items():
                    if not k.startswith("class_"):
                        continue
                    class_name = WorldCover2020.legend[int(k.replace("class_", ""))]
                    tile_stats[tile]["classes"][class_name] = v

            tar_stats[tar_folder] = dict(tile_stats)

        return tar_stats

    def create_webdataset_tars(self, tiles: List[str]):
        logger.info("Loading tile geometries")
        df = gpd.read_file(utils.data_dir / "esa_worldcover_2020_grid.geojson")

        text = ""
        for tile in tiles:
            geom_series = df[df["ll_tile"] == tile].geometry
            assert len(geom_series) == 1, f"Expected 1 geometry for {tile}"
            text += self._create_webdataset_tar_from_big_polygon(
                polygon=geom_series.iloc[0], prefix=tile
            )

        with open(utils.data_dir / "tile_processing.txt", "w") as f:
            f.write(text)

        tile_stats = self._generate_stats()
        with open(utils.data_dir / "tile_stats.yaml", "w") as yaml_file:
            yaml.dump(tile_stats, yaml_file, default_flow_style=False)

    def _tuples_from_decoded_files(self, iter: Iterable[Dict[str, np.ndarray]]) -> Iterable[Tuple]:
        for x in iter:
            start_month = random.choice(list(range(12)))
            eo = self.eo_pipeline.normalize(x["s1_s2_era5_srtm_2020_2021.npy"])
            latlons = x["latlon.npy"].astype(np.float32)
            worldcover_labels = self.worldcover_pipeline.normalize(x["worldcover2020.npy"])
            dw_labels = self.dynamicworld_pipeline.normalize(x["dynamicworld2020_2021.npy"])
            eo = self.clip(eo, start_month)
            dw_labels = pad_array(self.clip(dw_labels, start_month), NUM_TIMESTEPS)
            for eo_data, latlon, wc_label, dw_label in zip(
                eo, latlons, worldcover_labels, dw_labels
            ):
                if self.mask_params is None:
                    yield eo_data, dw_label, start_month, latlon, wc_label
                else:
                    mask_eo, mask_dw, x_eo, y_eo, x_dw, y_dw, strat = self.mask_params.mask_data(
                        eo_data, dw_labels
                    )
                    yield (
                        mask_eo,
                        mask_dw,
                        x_eo,
                        y_eo,
                        x_dw,
                        y_dw,
                        start_month,
                        latlon,
                        strat,
                        wc_label,
                    )


class S1_S2_ERA5_SRTM_DynamicWorldMonthly_2020_2021(Dataset):
    def __init__(self, mask_params: Optional[MaskParams] = None):
        self.eo_pipeline = S1_S2_ERA5_SRTM_2020_2021()
        self.dynamicworld_pipeline = DynamicWorldMonthly2020_2021()
        self.mask_params = mask_params
        super().__init__(pipelines=(self.eo_pipeline, self.dynamicworld_pipeline))

    def create_webdataset_tars(self, shard_ids: List[int] = [0]):
        logger.info("Loading Dynamic World geometries")
        gdf = gpd.read_file(utils.data_dir / "dynamic_world_samples.geojson")

        if "shard" not in gdf.columns:
            np.random.seed(0)
            n_shards = len(gdf) // N_RECORDS_IN_SHARD
            gdf["shard"] = np.random.choice(a=n_shards, size=len(gdf), replace=True)
            gdf.to_file(utils.data_dir / "dynamic_world_samples.geojson", driver="GeoJSON")

        text = ""
        for i in shard_ids:
            polygons = gdf[gdf["shard"] == i].geometry.to_list()
            polygon_ids = gdf[gdf["shard"] == i]["dw_id"].to_list()
            text += self._create_webdataset_tar_from_small_polygons(
                polygons=polygons,
                polygon_ids=polygon_ids,
                prefix=f"dw_{N_RECORDS_IN_SHARD}_shard_{i}",
            )

        logger.info("Saving logs and geojson")
        with open(utils.data_dir / "shard_processing.txt", "w") as f:
            f.write(text)

        active_shards = gdf[gdf["shard"].isin(shard_ids)].copy()

        active_shards["geometry"] = active_shards.to_crs("+proj=cea").centroid.to_crs(gdf.crs)
        active_shards.to_file(
            utils.data_dir / "dynamic_world_samples_active_shards.geojson", driver="GeoJSON"
        )

    def _tuples_from_decoded_files(self, iter: Iterable[Dict[str, np.ndarray]]) -> Iterable[Tuple]:
        for x in iter:
            # EO data is occasionally not available from Earth Engine
            if (
                ("s1_s2_era5_srtm_2020_2021.npy" not in x)
                or ("dynamicworld2020_2021.npy" not in x)
                or ("latlon.npy" not in x)
            ):
                continue
            start_month = random.choice(list(range(12)))
            eo = self.eo_pipeline.normalize(x["s1_s2_era5_srtm_2020_2021.npy"])
            latlons = x["latlon.npy"].astype(np.float32)
            dw_labels = self.dynamicworld_pipeline.normalize(x["dynamicworld2020_2021.npy"])
            eo = self.clip(eo, start_month)
            dw_labels = pad_array(self.clip(dw_labels, start_month), NUM_TIMESTEPS)
            for eo_data, latlon, dw_label in zip(eo, latlons, dw_labels):
                if self.mask_params is None:
                    yield eo_data, dw_label, start_month, latlon
                else:
                    mask_eo, mask_dw, x_eo, y_eo, x_dw, y_dw, strat = self.mask_params.mask_data(
                        eo_data, dw_label
                    )
                    yield MaskedExample(
                        mask_eo,
                        mask_dw,
                        x_eo,
                        y_eo,
                        x_dw,
                        y_dw,
                        start_month,
                        latlon,
                        strat,
                    )
